---
title: "Factor Analysis and Psychometrics of Subjective Distress among U.S. Graduate Students"
runningheader: "DeYoung et al. 2020" # only for pdf output
author: "Matthew Barstead, Ph.D."
date: "`r Sys.Date()`"
output:
  tufte::tufte_handout:
    includes: 
      in_header: preamble.tex
    highlight: tango
  tufte::tufte_html: default
  tufte::tufte_book:
    highlight: tango
  pdf_document:
    includes: 
      in_header: preamble.tex
bibliography: 
  - ACHA_factor_analysis.bib
  - ACHA_factor_analysis_Jrtcl.bib
nocite: '@*'
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE)
options(knitr.kable.NA = '')
library(ggraph)
library(glue)
library(tidyverse)
library(tidygraph)
library(tufte)

# Don't forget the DATA_VERSION update if needed
DATA_VERSION <- "2021-01-15"

source('~/github/ATNL/grad_mh/project_config.R')
sapply(list.files(R_DIR, full.names = TRUE), source)
load('{DATA_DIR}/ACHA-II/acha_grad_students_base_{DATA_VERSION}.RData' %>% glue())

set.seed(19151125)
train_rows <- sample(1:nrow(grads_only_study_df), size = floor(nrow(grads_only_study_df) * .7))
grad_train <- grads_only_study_df[train_rows,]
grad_test <- grads_only_study_df[-train_rows,]
```

# Overview
These analyses explored the degree to which the NHCA graduate student data supports creating one or more composite scores using responses to these 11 original items: `r paste0("NQ30", LETTERS[1:11]) %>% paste(collapse = ", ")`. Responses to these items were recorded on an ordinal scale with lower values corresponding to having never felt the target emotion or engaged in the indicated behavior and higher values corresponding to having experienced the emotion or performed the act recently (within the past two weeks). The rank order used for these items is displayed below^[For information about the transformations from the raw items to those used in these analyses see the `ACHA_data_QA.Rmd` file located [here](https://github.com/dr-consulting/grad_mh/blob/main/Rmd_files/ACHA_data_QA.Rmd)]: 

1. No, never
2. No, not in the last 12 months
3. Yes, in the last 12 months
4. Yes, in the last 30 days
5. Yes, in the last 2 weeks

# Method
The analyses involved multiple phases several of which were exploratory in nature. To mitigate the risk of overfitting in response to information gleaned during the exploratory stage we split the data set at the out set. Approximately 70% of the cases were targeted as the "training" set^["Training" and "testing" are common parlance in the machine learning world. Oversimplified the analyst builds the model on the training set and then sees how it fits to unobserved data in the testing set.]. The remaining 30% were held out for final evaluation of the measurement models. 

In the first stage of the analysis, we explored the potential invariance of inter-item correlations over time. The relative stability of inter-item correlations across survey periods provides necessary but insufficient evidence that we are measuring the same latent dimension(s).^[Finding basic support for invariance of relevant psychometric properties over time is not sufficient on its own to _prove_ that any composites or factor scores created assess the same phenomenon over time. But it is a good start.] 

In the second stage of the analysis, we focused on identifying the optimal number of factors to retain from the items. To help guide decision-making around factor structure (what items load on what latent factor, how many factors, etc.), we relied on principal components analyses, using Horn's parallel analysis to make factor retention decisions.

In the third stage og the analysis, we leveraged the results of our exploratory analyses and fit the data using a confirmatory factor analysis framework. Importantly, we were able to maintain the ordinality of the input data (all items were rated on a 5-point ordinal scale) when conducting these analyses. These structural-equation-based models provided the first empirical tests _wrt_ both the number of factors to retain and their relative invariance over time.   

Finally, in the fourth stage of the analysis, we moved to testing our model using the hold out "testing" data set. If we do choose to go down this path both this stage and the preceding one will expand. For the moment, a simple "sanity check" analysis is presented in the relevant section of the document and it does rely upon the test data. 

\pagebreak

# Phase I - Exploratory Psychometrics

There are a total of 22 separate surveys contained within the aggregated ACHA-NCHA data set. The analyses below provide insight into the overall pattern of associations among our ordinally transformed `Q30` variables. Our initial expectations were that somewhere between 1 and 3 factors may emerge, based on initial exploratory analyses. 

```{r, fig.height=4, fig.cap="Correlation network for complete training sample, across all survey time points. Correlations computed using Spearman's rank order method and only values > .5 are displayed. Thicker, denser lines represent stronger correlations. Three colors based on assumed categories of emotional distress (blue), stress (purple), and suicidality (green)"}
vars <- grads_only_study_df %>% 
    select(starts_with('Q30') & ends_with('_r')) %>% 
    colnames()

label_map <- c("Hopeless", 'Overwhelmed', "Exhausted", "Lonely", "Sad", "Depressed", "Anxiety", "Anger", "Self-Harm", 
               "Suicidal Thoughts", "Suicide Attempt")

names(label_map) <- vars

emo_distress <- c("Hopeless", "Lonely", "Sad", "Depressed", "Anxiety", "Anger")
high_stress <- c('Overwhelmed', "Exhausted")
suicidality <- c("Self-Harm", "Suicidal Thoughts", "Suicide Attempt")

color_map <- c(
  rep(analogous_palette[1], 6), # for 6 emotional distress items
  rep(analogous_palette[2], 2), # for 2 stress items
  rep(analogous_palette[3], 3) # for 3 suicide & self-harm items
)

names(color_map) <- c(emo_distress, high_stress, suicidality)

cor_method <- "spearman"
cor_cutoff <- .5

create_cor_network_plot(grad_train, vars, cor_method, label_map, color_map, cor_cutoff)
```

\newthought{Figure 1 suggests} that, at least when using a cutoff value of $|r| \ge .5$, there may be three factors that underlie the data. In the full training data, there is complete separation at this threshold among what we suspected could be three factors tapping emotional distress, stress, and suicidality. While this pattern may emerge in the aggregate, it is not necessarily true that it would be consistently true across time. 

Thus, we next binned our training data into 4, approximately equally sampled epochs, using the survey collection dates. The results are displayed in Figure 2. While there is some indication that slight variations did emerge across the 4 randomly divided epochs, by and large the same thresholded correlation network, replicated in each time window. 

```{r, fig.height=10, fig.fullwidth=TRUE, fig.width=7.5, fig.cap="Correlation networks for training sample split by 4 epochs designed to return approximately equally sized bins. There is some overlap in sequential epochs. For instance, the first epoch is Fall of 2008 to Spring of 2011 (F08-S11) and the second epoch is Spring of 2011 to Spring of 2014 (S11-S14)."}
grad_train <- grad_train %>% 
  mutate(
    epochs = ntile(time_point, 4)
  )

g1 <- create_cor_network_plot(grad_train %>% 
                                filter(epochs == 1), 
                              vars, cor_method, label_map, color_map, cor_cutoff)

g2 <- create_cor_network_plot(grad_train %>% 
                                filter(epochs == 2), 
                              vars, cor_method, label_map, color_map, cor_cutoff)

g3 <- create_cor_network_plot(grad_train %>% 
                                filter(epochs == 3), 
                              vars, cor_method, label_map, color_map, cor_cutoff)

g4 <- create_cor_network_plot(grad_train %>% 
                                filter(epochs == 4), 
                              vars, cor_method, label_map, color_map, cor_cutoff)

cowplot::plot_grid(g1, g2, g3, g4, nrow = 2, labels = c('F08-S11', 'S11-S14', 'S14-F16', 'F16-S19'))
```

One interesting implication of Figure 2 is that, over time, the association between variables appears to be growing stronger on average (i.e., we observe thicker and denser lines). This intensification of concurrent associations among risk factors for and symptoms of mental health disorder is an intriguing pattern in and of itself. 

\pagebreak

\newthought{Probing slightly deeper} into the factor structure, univariate and bivariate distributions are presented in Figures 4 through 6. As a reminder, higher scores mean that respondents more recently felt the target emotion or engaged in the described behavior. 

One clearly observable trend is that, even on an ordinal scale and with the exception of the suicidality variables, the bivariate associations are effectively linear in nature^[Figures 4-6 did not present any traditional scatterplots to visually describe the bivariate distributions. The ordinal data and the large sample size effectively returned a series of 5 x 5 grids with points perfectly overlayed on top of one another. A 2-dimensional heatmap is probably a better tool for observing the bivariate densities. See the `geom_bin2d` function as a starting point for 2D density plots.]. Another notable feature of these data is that the suicidality and self-harm items have fairly low base rates, which is not that surprising. 

\pagebreak

```{r, fig.height=10, fig.fullwidth=TRUE, fig.width=7.5, fig.cap="Univariate and bivariate distributions of the proposed emotional distress items. A 95-percent ellipse and a loess regression line are presented to visually depict bivariate associations"}

grad_train %>% 
  rename_with(.fn = function(x){label_map[x]}, all_of(names(label_map))) %>% 
  select(all_of(emo_distress)) %>% 
  psych::pairs.panels(
    hist.col = base_palette[1], 
    show.points = FALSE, 
    method = 'spearman', 
    density = FALSE, 
    rug = FALSE
  )
```

\pagebreak 

```{r, fig.height=3.35, fig.align="left", fig.cap="Univariate and bivariate distributions of the proposed stress items. A 95-percent ellipse and a loess regression line visually depict bivariate associations"}

grad_train %>% 
  rename_with(.fn = function(x){label_map[x]}, all_of(names(label_map))) %>% 
  select(all_of(high_stress)) %>% 
  psych::pairs.panels(
    hist.col = base_palette[1], 
    show.points = FALSE, 
    method = 'spearman', 
    density = FALSE, 
    rug = FALSE
  )
```

\pagebreak

```{r, fig.height=5, fig.align="left", fig.cap="Univariate and bivariate distributions of the proposed suicidality items. A 95-percent ellipse and a loess regression line visually depict bivariate associations"}

grad_train %>% 
  rename_with(.fn = function(x){label_map[x]}, all_of(names(label_map))) %>% 
  select(all_of(suicidality)) %>% 
  psych::pairs.panels(
    hist.col = base_palette[1], 
    show.points = FALSE, 
    method = 'spearman', 
    density = FALSE, 
    rug = FALSE
  )
```

\newthought{At this stage}, there is some tentative support for the existence of a set of latent variables that underlie the observed scores on this set of 11 items. The latent variables may be best conceptualized as an indicator of degree to which certain _feelings of_ or _behaviors related to_ subjective distress were recently felt. It is tempting to take it a step further and conceptualize the existence of potential latent factors as mapping onto severity. All else being equal, more severely distressed individuals are more likely to have experienced elevated distress more recently, but we should not assume a 1:1 conceptual mapping between recency and severity. 

\pagebreak

# Phase II - PCA and Invariance Testing

\newthought{Horn's Parallel Analysis} is a principled way of detecting latent factors in observed data. Instead of using arbitrary cutoffs, we simulate uncorrelated data sets with the same dimensionality as the target data. Then we extract eigenvalue distributions from each principal component. Lastly, we "walk" down the eigenvalues of the observed principal components, moving from the largest to the smallest. If the $i$th eigenvalue dips below the corresponding eigenvalue extracted from the simulated data's $i$th component, we stop and "retain" all components preceding the detection of the lower observed eigenvalue^[*Note*. The logic is similar to a using the elbow of scree plot. The difference is that we have a null distribution basis for creating the cutoffs as opposed to relying on visual interpretations of graphical outputs]. 

We'll applied this approach to each epoch separately, again as a means of verifying that whatever latent structure exists it is reasonably consistent over the entire data collection period. 

\pagebreak

\newthought{Results from epoch 1} suggested a 3-factor structure with items comprising a *negative emotionality* factor, *a stress/exhuastion* factor, and a *suicidality/self-harm* factor. The results align with the pattern of associations revealed by applying a cutoff of $|r| \ge .5$ to a correlation matrix^[See correlation network plots for a visual summary]. 

```{r}
res_epoch_1 <- 
grad_train %>% 
  filter(epochs == 1) %>% 
  rename_with(.fn = function(x){label_map[x]}, all_of(names(label_map))) %>% 
  select(all_of(c(emo_distress, high_stress, suicidality))) %>%
  mutate_all(as.numeric) %>% 
  pca_w_horns_pa(tbl_cap_base = "Standardized loadings returned by a principal components analysis of the epoch 1 observations.")
```

```{r, fig.cap="Scree plot of principal components detected in the epoch 1 observation set plotted alongside the eigenvalues of components from a parallel analysis that simulated 1000 uncorrelated data sets of equivalent dimensionality. We retained all components with eigenvalues greater than those generated from the uncorrelated simulated data."}
res_epoch_1$scree_plot
```

```{r}
res_epoch_1$latex_table
```

\pagebreak

\newthought{Results from epoch 2} were in line with the principal components analysis performed on the epoch 1 training data. Again 3 latent dimensions were retained, and again there were no cross-loadings $\ge .3$ in the rotated factor space. All in all, these findings are consistent with the existence of relatively invariant latent dimensions - but not conclusive evidence. 

```{r}
res_epoch_2 <- 
grad_train %>% 
  filter(epochs == 2) %>% 
  rename_with(.fn = function(x){label_map[x]}, all_of(names(label_map))) %>% 
  select(all_of(c(emo_distress, high_stress, suicidality))) %>%
  mutate_all(as.numeric) %>% 
  pca_w_horns_pa(tbl_cap_base = "Standardized loadings returned by a principal components analysis of the epoch 2 observations.")
```

```{r, fig.cap="Scree plot of principal components detected in the epoch 2 observation set plotted alongside the eigenvalues of components from a parallel analysis that simulated 1000 uncorrelated data sets of equivalent dimensionality. We retained all components with eigenvalues greater than those generated from the uncorrelated simulated data."}
res_epoch_2$scree_plot
```

```{r}
res_epoch_2$latex_table
```

\pagebreak

\newthought{Results from epoch 3} suggested the existence of only 2 factors. The caveat to keep in mind here is that these analyses are not confirmatory (nor are they disconfimatory). In fact, the failure to retain a third factor is best categorized as a near miss^[See **Table 4** ]. The difference between the observed eigenvalue of a third component and its cut point at the 95th percentile was < .001. Still, it is hard to refer to these results as being _consistent_ with the hypothesis that latent dimensions are relatively invariant in their structure over time. 

```{r}
res_epoch_3 <- 
grad_train %>% 
  filter(epochs == 3) %>% 
  rename_with(.fn = function(x){label_map[x]}, all_of(names(label_map))) %>% 
  select(all_of(c(emo_distress, high_stress, suicidality))) %>%
  mutate_all(as.numeric) %>% 
  pca_w_horns_pa(tbl_cap_base = "Standardized loadings returned by a principal components analysis of the epoch 3 observations.")
```

```{r, fig.cap="Scree plot of principal components detected in the epoch 3 observation set plotted alongside the eigenvalues of components from a parallel analysis that simulated 1000 uncorrelated data sets of equivalent dimensionality. We retained all components with eigenvalues greater than those generated from the uncorrelated simulated data."}
res_epoch_3$scree_plot
```

```{r}
res_epoch_3$latex_table
```

\pagebreak

```{r}
knitr::kable(res_epoch_3$pa_results[1:4,], booktabs = TRUE, format = "latex", 
             caption = str_wrap("First four components of the epoch 3 parallel analysis. PA = parallel analysis."))
```

\pagebreak

\newthought{Results from epoch 4} once again highlight the fact that the third factor - a separate dimension for the overwhelmed and exhausted items on the cusp of the retention criteria^[See **Table 6**]. In the end, two epochs suggested the presence of three factors (epochs 1 and 2) and two epochs did not (epochs 3 and 4). 

In the next section we attempt to use a structural equation modeling approach to 1) compare alternative factor structures (2 vs. 3) and 2) assess measurement invariance over time. 

```{r}
res_epoch_4 <- 
grad_train %>% 
  filter(epochs == 4) %>% 
  rename_with(.fn = function(x){label_map[x]}, all_of(names(label_map))) %>% 
  select(all_of(c(emo_distress, high_stress, suicidality))) %>%
  mutate_all(as.numeric) %>% 
  pca_w_horns_pa(tbl_cap_base = "Standardized loadings returned by a principal components analysis of the epoch 4 observations.")
```

```{r, fig.cap="Scree plot of principal components detected in the epoch 4 observation set plotted alongside the eigenvalues of components from a parallel analysis that simulated 1000 uncorrelated data sets of equivalent dimensionality. We retained all components with eigenvalues greater than those generated from the uncorrelated simulated data."}
res_epoch_4$scree_plot
```

```{r}
res_epoch_4$latex_table
```

```{r}
knitr::kable(res_epoch_3$pa_results[1:4,], booktabs = TRUE, format = "latex", 
             caption = str_wrap("First four components of the epoch 4 parallel analysis. PA = parallel analysis."))
```

\pagebreak

# Phase III - Training Sample Multigroup CFA

This set of analyses attempts to address two questions that could not be resolved using the exploratory results to this point. The first is whether 2 or 3 latent factors provide a better description of the data. The second is whether the better fitting of the two solution yields relatively invariant latent dimensions over time. 

We'll address both questions by:

1. Creating a new set of epochs based on survey time point^[The previous analyses that split out results based on epoch allowed for some spillover across survey data collected at each epoch's boundaries.].
2. Completing a series of measurement model invariance tests on the best fitting of the two models with the new `survey_epoch` variable serving as the "grouping" variable^[*Note*. For these analyses, we are explicitly maintaining the ordinal measurement property of the observed variables, necessitating the use of a diagonally weighted least squares estimator. See the [lavaan tutorial](https://lavaan.ugent.be/tutorial/est.html) for a brief overview]. 

```{r}
grad_train <- grad_train %>% 
  mutate(
    survey_epoch = case_when(
      time_point <= "2011-04-01" ~ 1,
      time_point > "2011-04-01" & time_point <= "2014-04-01" ~ 2, 
      time_point > "2014-04-01" & time_point <= "2016-10-01" ~ 3, 
      time_point > "2016-10-01" ~ 4
    )
  )


grad_test <- grad_test %>% 
  mutate(
    survey_epoch = case_when(
      time_point <= "2011-04-01" ~ 1,
      time_point > "2011-04-01" & time_point <= "2014-04-01" ~ 2, 
      time_point > "2014-04-01" & time_point <= "2016-10-01" ~ 3, 
      time_point > "2016-10-01" ~ 4
    )
  )
```

```{r}
library(lavaan)

two_fact_base <- "
F1 =~ Q30A_hopeless_r + Q30D_lonely_r + Q30E_sad_r + Q30F_depressed_r + Q30G_anxiety_r + Q30H_anger_r + Q30B_overwhelmed_r + Q30C_exhausted_r
F2 =~ Q30I_selfharm_r + Q30J_suic_thnk_r + Q30K_suic_try_r 
"

three_fact_base <- "
F1 =~ Q30A_hopeless_r + Q30D_lonely_r + Q30E_sad_r + Q30F_depressed_r + Q30G_anxiety_r + Q30H_anger_r 
F2 =~ Q30I_selfharm_r + Q30J_suic_thnk_r + Q30K_suic_try_r 
F3 =~ Q30B_overwhelmed_r + Q30C_exhausted_r
"
```

\pagebreak

\newthought{A baseline comparison} of a 2-factor multigroup to a 3-factor multigroup confirmatory factor analysis revealed support for the 3-factor model over the 2-factor model. See **Table  7** for a formal $\chi^2$ difference test. 

```{r}
fit_two_fact_base <- cfa(two_fact_base, data = grad_train, ordered = vars, group = "survey_epoch")
fit_three_fact_base <- cfa(three_fact_base, data = grad_train, ordered = vars, group = "survey_epoch")
anv_base_tbl <- anova(fit_two_fact_base, fit_three_fact_base)
anv_base_tbl <- anv_base_tbl[,c('Df', 'Chisq', 'Chisq diff', 'Pr(>Chisq)')]
rownames(anv_base_tbl) <- c("Three Factor", "Two Factor")

knitr::kable(anv_base_tbl, booktabs = TRUE, format = "latex", 
             caption = str_wrap("Model comparison of a multigroup 2-factor model and a multigroup 3-factor model. A significant test indicates that removal of paths in the simpler model resulted in worse data-model fit. For both, models no equality constraints were placed on the model estimates across survey epoch groupings."))
```

```{r}
mod_matrix <- inspect(fit_three_fact_base, what = "std")

F1_loadings <- data.frame(Variable = emo_distress) 
F2_loadings <- data.frame(Variable = suicidality)
F3_loadings <- data.frame(Variable = high_stress)

for(i in seq_along(mod_matrix)){
  F1_loadings[[paste("Epoch", i)]] <- mod_matrix[[i]]$lambda[,"F1"][mod_matrix[[i]]$lambda[,"F1"] > 0] %>% 
    round(digits = 3)
  F2_loadings[[paste("Epoch", i)]] <- mod_matrix[[i]]$lambda[,"F2"][mod_matrix[[i]]$lambda[,"F2"] > 0] %>% 
    round(digits = 3)
  F3_loadings[[paste("Epoch", i)]] <- mod_matrix[[i]]$lambda[,"F3"][mod_matrix[[i]]$lambda[,"F3"] > 0] %>% 
    round(digits = 3)
}
```

We see in Table 7, for the emotional distress latent factor, the magnitude of the loadings across the four survey epochs is consistent with the size of the loadings presented in the principal components analyses above. Also noteworthy, for each of the 6 variables expected to load on this latent dimension, the standardized coefficients inhabit a relatively narrow range. For instance, there are no variables that load at .5 for one of the survey epochs and then at .75 for another. 

```{r}
knitr::kable(F1_loadings, booktabs = TRUE, format = "latex", 
             caption = str_wrap("Standardized factor loadings for Factor 1, separately for each of the four survey epochs"))
```

Again, as seen in Table 8, the results for the suicdiality items revealed a similar pattern to the one detected using the principal components analyis above. We also see a relatively narrow range across epochs.

```{r}
knitr::kable(F2_loadings, booktabs = TRUE, format = "latex", 
             caption = str_wrap("Standardized factor loadings for Factor 2, separately for each of the four survey epochs."))
```

By and large, we also found that loadings for the latent variable representing a sense of exhaustion were similar in magnitude to those found in the principal components analyses ^[*Note*. This is true only of epochs 1 and 2. The component representing this dimension of the variable space did not exceed the 95th percentile in epochs 3 or 4]. The loadings were also similar within this confirmatory factor analysis across survey epochs. 

```{r}
knitr::kable(F3_loadings, booktabs = TRUE, format = "latex", 
             caption = str_wrap("Standardized factor loadings for Factor 3, separately for each of the four survey epochs. Includes a pair of items that tap the recency feeling overwhelmed and exhausted."))
```

\pagebreak

\newthought{Measurement invariance} is the idea if a latent dimension is measuring the same thing across a grouping level then we should see evidence that components of the model fit to each of those groupings (parameter estimates, variance terms, etc.) are approximately equal. Or, at least in the formal testing approach used here, we should expect that constraining those components to be equal should not significantly reduce data-model fit. 

```{r}
fit_three_fact_metric <- cfa(three_fact_base, data = grad_train, ordered = vars, group = "survey_epoch", 
                         group.equal=c('loadings'))

anv_base_tbl <- anova(fit_three_fact_metric, fit_three_fact_base)
anv_base_tbl <- anv_base_tbl[,c('Df', 'Chisq', 'Chisq diff', 'Pr(>Chisq)')]
rownames(anv_base_tbl) <- c("Configural", "Metric")

knitr::kable(anv_base_tbl, booktabs = TRUE, format = "latex", 
             caption = str_wrap("Model comparison of a configural, 3-factor, multigroup confirmatory factor analysis against a version of the same model with metric constraints applied. Configural and metric are technical terms in this case. The former indicates a model with no equality constraints enforced across the grouping factor. The latter indicates a model with equality constraints placed on the loadings and thresholds in the case of a model that includes ordinal observed variables. A significant test indicates that removal of paths in the simpler model (the metric invariance model) resulted in worse data-model fit."))
```

The simplest form of measurement invariance is sometimes referred to as "metric" invariance^[See Putnick & Bornstein, *Developmental Review*, 2016 for an accessible overview]. Metric invariance is established when we can constrain all loadings from the observed variables onto the latent variables to be equal across groups without paying any penalties in terms of data-model fit. More formally, if the $\chi^2$ test of model differences between the constrained and unconstrained model is significant it suggests that freeing the paths to take on different values across grouping levels _improved_ fit. When that happens the standard of metric invariance is not met. 

As shown in Table 11, we find evidence *against the hypothesis that these three factors meet the criteria of metric invariance. Model fit is worse when we constrain all of the loadings to be equal across each of the 4 epochs. That does not mean that we have the wrong model, but rather that we may need to consider that in creating any latent scores from these data, we would benefit from incorporating any non-equivalent paths in our final model. This is an iterative process beyond the scope of this exploratory document. Because we have not been able to establish metric invariance, there is little point in continuing forward with testing of even more restrictive forms of invariance (i.e., scalar and residual). 

\pagebreak

# Phase IV - Out of Sample CFA

\newthought{Having found more} robust support for the three-factor model than the two-factor model in our data, our final step is a simple out of sample confirmation that the model fits the observed data in our hold out test sample. If we proceed with determining the sources of metric invariance, we can eventually leverage this hold out data to protect against overfitting by ensuring the final model is the one that performs the best when applied to the holdout sample. 
```{r}
fit_three_test <- cfa(three_fact_base, data = grad_test, ordered = vars)
mod_matrix <- inspect(fit_three_test, what = "std")

rownames(mod_matrix$lambda) <- c(emo_distress, suicidality, high_stress)
mod_matrix$lambda[mod_matrix$lambda == 0] <- NA
knitr::kable(round(mod_matrix$lambda, digits = 3), booktabs = TRUE, format = "latex", 
             caption = str_wrap("Loadings from a full sample 3-factor model tested using the hold out sample. There was no grouping factor in this case, and this model is merely a simple sanity check at this stage. If we go ahead with the derivation of factor scores - this hold out sample will be a useful backstop against the risk of overfitting to our training data when attempting to account for sources of measurement invariance."))
```

\pagebreak

```{r bib, include=FALSE}
# create a bib file for the R packages used in this document
knitr::write_bib(c('base', 'rmarkdown', 'lavaan', 'tidyverse', 'psych', 'nFactors'), file = 'ACHA_factor_analysis.bib')
```
