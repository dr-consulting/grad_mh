---
title: "Supplement: Factor Analysis and Psychometrics of Subjective Distress among U.S. Graduate Students"
runningheader: "DeYoung et al. 2023" # only for pdf output
author: "Matthew Barstead, Ph.D."
date: "`r Sys.Date()`"
output:
  tufte::tufte_handout:
    includes: 
      in_header: preamble.tex
    highlight: tango
  tufte::tufte_html: default
  tufte::tufte_book:
    highlight: tango
  pdf_document:
    includes: 
      in_header: preamble.tex
bibliography: 
  - ACHA_factor_analysis.bib
  - ACHA_factor_analysis_Jrtcl.bib
nocite: '@*'
editor_options:
  chunk_output_type: console
urlcolor: blue
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE, cache = TRUE)
options(knitr.kable.NA = '')
library(ggraph)
library(glue)
library(tidyverse)
library(tidygraph)
library(tufte)
library(lavaan)

BASE_FILE <- '~/Desktop/grad_mh/project_config.R'
DATA_VERSION <- '2021-02-04'

# If missing the config file raise early.
# Likely just opened the repo in a different file system
if(!file.exists(BASE_FILE)){
    stop('ERROR: Missing project config file. {BASE_FILE}' %>% glue())
}

source(BASE_FILE)
sapply(list.files(R_DIR, full.names = TRUE), source)

load('{DATA_DIR}/ACHA-II/acha_grad_students_base_{DATA_VERSION}.RData' %>% glue())

set.seed(19151125)
train_rows <- sample(1:nrow(grads_model_base), size = floor(nrow(grads_model_base) * .7))
grad_train <- grads_model_base[train_rows,]
grad_test <- grads_model_base[-train_rows,]
```

# Overview
Here we present a series of correlational and factor analyses that evaluates the potential latent dimensionality underyling a set of emotional distress items collected in the National Collegiate Health Assessment (NCHA; $N = 189,099$) semi-annual survey. We focused on 8 items tapping: hopelessness, feeling overwhelmed, exhaustion, loneliness, sadness, depressed mood, anxious mood, and anger.^[Items tapping suicidality and self-harm were also part of this set of questions. However, we were interested in analyzing those data separately.] NCHA respondents rated how recently they had felt the emotional states described in the survey items using a 5-point ordinal scale, which we re-coded such that lower values indicated less recent instances of the described state and higher values indicated more recent instances.^[For information about the transformations from the raw items to those used in these analyses see the `ACHA_data_QA.Rmd` file located [here](https://github.com/dr-consulting/grad_mh/blob/main/Rmd_files/ACHA_data_QA.Rmd)] 


## Response Options

1. No, never
2. No, not in the last 12 months
3. Yes, in the last 12 months
4. Yes, in the last 30 days
5. Yes, in the last 2 weeks

# Method
The analyses involved multiple phases, several of which were exploratory in nature. To mitigate the risk of over-fitting by using information gleaned during the exploratory stage, we split the data set at the outset. Approximately 70% of the cases were targeted as the "training" set^["Training" and "testing" are common parlance in the machine learning world. Oversimplified, the analyst builds the model on the training set and then evaluates fit when applied to unobserved data in the testing set.]. The remaining 30% were held out for final evaluation of the measurement models. 

In the first stage of the analysis, we explored the potential invariance of inter-item correlations over time. The relative stability of inter-item correlations across survey periods provides necessary but insufficient evidence that the same latent dimensions persist over time.^[Finding basic support for invariance of relevant psychometric properties is not sufficient on its own to _prove_ that any composites or factor scores created assess the same phenomenon over time. But, it is a good start.] 

In the second stage of the analysis, we focused on identifying the optimal number of factors to retain from the items. To help guide decision-making _wrt_ factor structure (e.g., what items load on what latent factor, how many factors, etc.), we relied on principal components analyses in conjunction with Horn's parallel analysis.

In the third stage of the analysis, we leveraged the results of our exploratory analyses and fit the data using a confirmatory factor analysis (CFA) framework. Importantly, we conserved the ordinal nature of the input data when conducting these analyses.^[All items were rated on a 5-point ordinal scale.] These structural-equation-based models provided the first null hypothesis tests _wrt_ the number of factors to retain and the latent variable structure more generally.

Finally, in the fourth stage of the analysis, we moved to testing the CFA models using the holdout "testing" data set. Specifically, we created a series of CFA models in which the paths, variances, and covariances were constrained to be equal to the estimates returned from the models fitted on the training data. We expected to find relatively minimal decrements in model fit for models with replicable latent structures (i.e., standard fit indices would be similar when the constrained models were fit to the hold out test data). 

\pagebreak

# Phase I - Exploratory Correlational Analyses

There were a total of 22 separate surveys contained within the aggregated ACHA-NCHA data set. The analyses below provide insight into the overall pattern of associations among our ordinally transformed `Q30` variables, which assessed the recency of distressing emotional states. Our initial expectations were that somewhere between 1 and 3 factors may emerge, based on initial exploratory analyses and previous experience working with similar item sets. 

```{r, fig.height=4, fig.cap="Correlation network for complete training sample, across all survey time points. Correlations computed using Spearman's rank order method and only values > .5 are displayed. Thicker, denser lines represent stronger correlations. Two colors based on assumed categories of negative emotionality (blue) and exhaustion (purple)"}
vars <- grads_model_base %>% 
  select(starts_with('Q30') & ends_with('_r')) %>% 
  # Excluding the suicide and self-harm items from the analysis
  select(-starts_with("Q30I"), -starts_with("Q30J"), -starts_with("Q30K")) %>% 
  colnames()

label_map <- c("Hopeless", 'Overwhelmed', "Exhausted", "Lonely", "Sad", "Depressed", "Anxiety", "Anger")

names(label_map) <- vars

emo_distress <- c("Hopeless", "Lonely", "Sad", "Depressed", "Anxiety", "Anger")
high_stress <- c('Overwhelmed', "Exhausted")

color_map <- c(
  rep(analogous_palette[1], 6), # for 6 emotional distress items
  rep(analogous_palette[2], 2)  # for 2 stress/exhaustion items
)

names(color_map) <- c(emo_distress, high_stress)

cor_method <- "spearman"
cor_cutoff <- .5

create_cor_network_plot(grad_train, vars, cor_method, label_map, color_map, cor_cutoff)
```

\newthought{Figure 1 suggests} that, at least when using a cutoff value of $|r| \ge .5$, there may be two factors that underlie the data. In the full training data^[Recall the training data is approximately 70% of the total available data], there is complete separation at this threshold among what we suspected could be two factors tapping _negative emotionality_ and _exhaustion_. While this pattern may emerge in the aggregate, it is not necessarily true that it would be consistently detected across the entire survey window (Fall of 2008 through the Spring of 2019). 

Thus, we next binned our training data into 4, approximately equally sampled epochs, using survey collection dates. The results are displayed in Figure 2. While there is some indication that slight variations did emerge across the 4 randomly divided epochs, by and large, the same thresholded correlation network replicated in each epoch. 

```{r, fig.height=10, fig.fullwidth=TRUE, fig.width=7.5, fig.cap="Correlation networks for training sample split by equally sized epochs. Note, there is some overlap in sequential epochs. For instance, the first epoch is Fall of 2008 to Spring of 2011 (F08-S11), and the second epoch is Spring of 2011 to Spring of 2014 (S11-S14)."}
grad_train <- grad_train %>% 
  mutate(
    epochs = ntile(time_point, 4)
  )

g1 <- create_cor_network_plot(grad_train %>% 
                                filter(epochs == 1), 
                              vars, cor_method, label_map, color_map, cor_cutoff)

g2 <- create_cor_network_plot(grad_train %>% 
                                filter(epochs == 2), 
                              vars, cor_method, label_map, color_map, cor_cutoff)

g3 <- create_cor_network_plot(grad_train %>% 
                                filter(epochs == 3), 
                              vars, cor_method, label_map, color_map, cor_cutoff)

g4 <- create_cor_network_plot(grad_train %>% 
                                filter(epochs == 4), 
                              vars, cor_method, label_map, color_map, cor_cutoff)

cowplot::plot_grid(g1, g2, g3, g4, nrow = 2, labels = c('F08-S11', 'S11-S14', 'S14-F16', 'F16-S19'))
```

One interesting implication of Figure 2 is that, over time, the association between variables appears to be growing stronger on average (i.e., we observe thicker and denser lines). This intensification of concurrent associations among emotional risk factors for mental health disorder is an intriguing pattern in and of itself, though not the target of our analyses.

\pagebreak

\newthought{Probing slightly deeper} into the item properties, univariate and bivariate distributions are presented in Figures 3 and 4. As a reminder, higher scores indicate that respondents more recently felt the target emotion. 

One clearly observable pattern is that, even with the discontinuities of an ordinal scale, the bivariate associations are effectively linear in nature^[Figures 3-4 do not include traditional scatterplots to visually describe the bivariate distributions. The ordinal data and the large sample size effectively returned a series of 5 x 5 grids with points perfectly overlayed on top of one another. A 2-dimensional heatmap is probably a better tool for observing the bivariate densities. See the `geom_bin2d` function as a starting point for 2D density plots.]. 

\pagebreak

```{r, fig.height=10, fig.fullwidth=TRUE, fig.width=7.5, fig.cap="Univariate and bivariate distributions of the proposed negative emotionality items. A 95-percent ellipse and a loess regression line are presented to visually depict bivariate associations"}

grad_train %>% 
  rename_with(.fn = function(x){label_map[x]}, all_of(names(label_map))) %>% 
  select(all_of(emo_distress)) %>% 
  psych::pairs.panels(
    hist.col = base_palette[1], 
    show.points = FALSE, 
    method = 'spearman', 
    density = FALSE, 
    rug = FALSE
  )
```

\pagebreak 

```{r, fig.height=3.35, fig.align="left", fig.cap="Univariate and bivariate distributions of the proposed exhaustion items. A 95-percent ellipse and a loess regression line visually depict bivariate associations"}

grad_train %>% 
  rename_with(.fn = function(x){label_map[x]}, all_of(names(label_map))) %>% 
  select(all_of(high_stress)) %>% 
  psych::pairs.panels(
    hist.col = base_palette[1], 
    show.points = FALSE, 
    method = 'spearman', 
    density = FALSE, 
    rug = FALSE
  )
```

\pagebreak

\newthought{At this stage}, there is some tentative support for the existence of a set of latent variables that underlie the observed scores on this set of 8 items. The latent variables may be best conceptualized as indicators of the degree to which certain _feelings of_ or _behaviors related to_ subjective distress were recently felt. It is tempting to take the description a step further and conceptualize the existence of potential latent factors as mapping onto severity. All else being equal, more severely distressed individuals are more likely to have experienced elevated distress more recently, but we should not assume a 1:1 conceptual mapping between recency and severity. 

\pagebreak

# Phase II - PCA and Invariance Assessment

\newthought{Horn's Parallel Analysis} is a principled technique for detecting latent factors in observed data. Instead of using arbitrary cutoffs^[For instance, retaining all components with an eigenvalue greater than 1.], we simulated uncorrelated data sets with the same dimensionality as the target data. Then, we extracted eigenvalue distributions from each principal component. Lastly, we "walked down" the eigenvalues of the observed principal components, moving from the largest to the smallest. If the $i$th eigenvalue dipped below the corresponding eigenvalue extracted from the simulated uncorrelated data's $i$th component, we stopped and "retained" all components preceding the detection of the lower observed eigenvalue^[*Note*. The logic is similar to using the elbow of scree plot. The difference is that we have a null distribution basis for creating the cutoffs as opposed to relying on visual interpretations of graphical outputs]. 

We applied this approach to each epoch separately, again as a means of verifying that whatever latent structure exists it is reasonably consistent over the entire data collection period. 

\pagebreak

\newthought{Results from epoch 1} suggested a 2-factor structure with items comprising a *negative emotionality* factor and an *exhaustion* factor. The results align with the pattern of associations revealed by applying a cutoff of $|r| \ge .5$ to a correlation matrix of the same item set. 

```{r}
res_epoch_1 <- 
grad_train %>% 
  filter(epochs == 1) %>% 
  rename_with(.fn = function(x){label_map[x]}, all_of(names(label_map))) %>% 
  select(all_of(c(emo_distress, high_stress))) %>%
  mutate_all(as.numeric) %>% 
  pca_w_horns_pa(tbl_cap_base = "Standardized loadings returned by a principal components analysis of the epoch 1 observations.")
```

```{r, fig.cap="Scree plot of principal components detected in the epoch 1 observation set plotted alongside the 95th percentile eigenvalues of components from a parallel analysis that simulated 1000 uncorrelated data sets of equivalent dimensionality."}
res_epoch_1$scree_plot
```

```{r}
res_epoch_1$latex_table
```

\pagebreak

\newthought{Results from epoch 2} were in line with the principal components analysis performed on the epoch 1 training data. Again 2 latent dimensions were retained, and, again, there were no cross-loadings $\ge .3$ in the rotated factor space. All in all, these findings are consistent with the existence of relatively invariant latent dimensions. 

```{r}
res_epoch_2 <- 
grad_train %>% 
  filter(epochs == 2) %>% 
  rename_with(.fn = function(x){label_map[x]}, all_of(names(label_map))) %>% 
  select(all_of(c(emo_distress, high_stress))) %>%
  mutate_all(as.numeric) %>% 
  pca_w_horns_pa(tbl_cap_base = "Standardized loadings returned by a principal components analysis of the epoch 2 observations.")
```

```{r, fig.cap="Scree plot of principal components detected in the epoch 2 observation set plotted alongside the 95th percentile eigenvalues of components from a parallel analysis that simulated 1000 uncorrelated data sets of equivalent dimensionality."}
res_epoch_2$scree_plot
```

```{r}
res_epoch_2$latex_table
```

\pagebreak

\newthought{Results from epoch 3} again suggested the existence of 2 factors. This repeated pattern offers additional evidence in favor of relatively invariant latent factors. 

```{r}
res_epoch_3 <- 
grad_train %>% 
  filter(epochs == 3) %>% 
  rename_with(.fn = function(x){label_map[x]}, all_of(names(label_map))) %>% 
  select(all_of(c(emo_distress, high_stress))) %>%
  mutate_all(as.numeric) %>% 
  pca_w_horns_pa(tbl_cap_base = "Standardized loadings returned by a principal components analysis of the epoch 3 observations.")
```

```{r, fig.cap="Scree plot of principal components detected in the epoch 3 observation set plotted alongside the 95th percentile eigenvalues of components from a parallel analysis that simulated 1000 uncorrelated data sets of equivalent dimensionality."}
res_epoch_3$scree_plot
```

```{r}
res_epoch_3$latex_table
```

\pagebreak

\newthought{Results from epoch 4} revealed the same 2-factor solution as the previous 3 epochs. 

```{r}
res_epoch_4 <- 
grad_train %>% 
  filter(epochs == 4) %>% 
  rename_with(.fn = function(x){label_map[x]}, all_of(names(label_map))) %>% 
  select(all_of(c(emo_distress, high_stress))) %>%
  mutate_all(as.numeric) %>% 
  pca_w_horns_pa(tbl_cap_base = "Standardized loadings returned by a principal components analysis of the epoch 4 observations.")
```

```{r, fig.cap="Scree plot of principal components detected in the epoch 4 observation set plotted alongside the 95th percentile eigenvalues of components from a parallel analysis that simulated 1000 uncorrelated data sets of equivalent dimensionality."}
res_epoch_4$scree_plot
```

```{r}
res_epoch_4$latex_table
```

\pagebreak

# Phase III - Training Sample CFA

The consistent detection of a 2-factor solution using a series of principal components analyses bolsters the claim that there are two, stable, latent dimensions captured by individuals' responses to the targeted set of 8 distress items: _negative emotionality_ and _exhaustion_. Principal components analyses do not, however, offer a means of directly testing whether a one-, two-, or three-factor solution better fits the observed data. Furthermore, the PCA analyses ignore the measurement properties of the scores - treating ordinal measures as continuous. On an exploratory basis, and at the current sample sizes, this blurring of measurement type is not likely to be problematic, but it is a minor limitation nonetheless. 

To address these two shortcomings and to probe the factor structure more deeply, we turned to structural equation modeling and confirmatory factor analyses (CFAs).^[*Note*. For these analyses, we are explicitly maintaining the ordinal measurement property of the observed variables, necessitating the use of a diagonally weighted least squares estimator. See the [lavaan tutorial](https://lavaan.ugent.be/tutorial/est.html) for a brief overview.] CFA models allow direct comparisons of factor structures when one of the proposed structures is a constrained version of another proposed structure (e.g., loadings constrained to be equal or set to 0). With "restricted" vs. "full" model comparisons, we can determine whether constraining or freeing a path leads to improved (or not worsened) data-model fit. 

A summary of our approach to this portion of analysis is: 

1. Determine whether a one-factor or two-factor solution better fits the data.
1. If a two-factor solution provides better data-model fit than a one-factor solution, use error covariances and modification indices to guide creation of a plausible three-factor solution. 
1. If a plausible and interpretable three-factor CFA model can be defined, run and compare against the one- and two-factor models. 
1. Consider reasonable alternative structures that could account for the observed data. 
1. Summarize findings and propose final model.

```{r}
grad_train <- grad_train %>% 
  mutate(
    survey_epoch = case_when(
      time_point <= "2011-04-01" ~ 1,
      time_point > "2011-04-01" & time_point <= "2014-04-01" ~ 2, 
      time_point > "2014-04-01" & time_point <= "2016-10-01" ~ 3, 
      time_point > "2016-10-01" ~ 4
    )
  )


grad_test <- grad_test %>% 
  mutate(
    survey_epoch = case_when(
      time_point <= "2011-04-01" ~ 1,
      time_point > "2011-04-01" & time_point <= "2014-04-01" ~ 2, 
      time_point > "2014-04-01" & time_point <= "2016-10-01" ~ 3, 
      time_point > "2016-10-01" ~ 4
    )
  )
```

\pagebreak

## One-Factor Model

We began our confirmatory factor analyses (CFAs) by creating a single-factor model. Both the exploratory correlation network analyses and the principal components analyses reported in the previous sections supported a two-factor structure. However, neither approach was capable of determining whether a two-factor approach outperformed a one-factor model in terms of accounting for observed correlations among items. Thus, our one-factor model represents our "baseline" model as the simplest factor structure we could apply to our data.^[Our primary interest was determining whether creating one or more composite values from these items was both empirically and thereotically justifiable. We had little to no interest in proving a particular factor structure fit these data better than another.] 

Fit statistics for the one-factor model suggest that it did not fit the data particularly well. Explicitly, we would expect to see $CFI > .95$ and $SRMR < .08$ in a model that provides a reasonable approximation of the latent structure. Neither was true in the one-factor model. Notably, we observed a significant $\chi^2$ statistic, denoted by the $p$ row in Table 5. In smaller samples, this result may be of concern. However, in a sample this large, even small deviations between the observed and expected covariance matrices can create large $\chi^2$ estimates. Throughout our CFA modeling, we mainly relied on robust $\chi^2$ difference tests to compare alternative models and otherwise ignored the standalone $\chi^2$ statistic as a meaningful indicator of overall data-model fit. 

```{r}
dich_vars <- c('Q30A_hopeless_r_2wks', 'Q30D_lonely_r_2wks', 'Q30E_sad_r_2wks', 'Q30F_depressed_r_2wks', 'Q30G_anxiety_r_2wks', 'Q30H_anger_r_2wks', 'Q30B_overwhelmed_r_2wks', 'Q30C_exhausted_r_2wks')

for(var in dich_vars) {
  grad_train[[var]] <- as.factor(grad_train[[var]])
  grad_train[[var]] <- ordered(grad_train[[var]])
  print(levels(grad_train[[var]]))
}

one_fact_base <- "
F1 =~ Q30A_hopeless_r_2wks + Q30D_lonely_r_2wks + Q30E_sad_r_2wks + Q30F_depressed_r_2wks + Q30G_anxiety_r_2wks + Q30H_anger_r_2wks + Q30B_overwhelmed_r_2wks + Q30C_exhausted_r_2wks
"
res_1 <- find_single_factor_final_model(one_fact_base, data = grad_train, ordered_vars = dich_vars, p_thresh = 1e-5)

# Get loadings for base model
mod_matrix <- inspect(res_1[["base_fit"]], what = "std")
F1_loadings <- mod_matrix$lambda[,"F1"][mod_matrix$lambda[,"F1"] != 0] %>% 
    round(digits = 3)
names(F1_loadings) <- c(emo_distress, high_stress)

# Get fit stats for base model
fit_stats <- c('chisq.scaled', 'df', 'pvalue.scaled', 'cfi.scaled', 'srmr')
fit_tbl <- fitmeasures(res_1[["base_fit"]], fit.measures = fit_stats) %>% 
  round(digits = 3)
names(fit_tbl) <- c('$\\chi^2$', '$df$', '$p$', 'CFI', 'SRMR')

# Write tables
knitr::kable(fit_tbl, booktabs = TRUE, format = "latex", escape = FALSE, col.names = "Value",
             caption = str_wrap("Fit statistics for one-factor CFA model, with no error covariance"))

knitr::kable(F1_loadings, booktabs = TRUE, format = "latex", col.names = c("F1"),
             caption = str_wrap("Standardized factor loadings for one-factor CFA model, with no error covariances"))
```

\pagebreak

\newthought{As a follow-up}, we conducted a series of exploratory analyses in which we added error covariances using model modification indices as a guide. Model modification indices are estimates of the incremental improvement in data-model fit when a proposed path is added to an existing model. Given the number of comparisons and the large sample size, we set a relatively strict threshold for retaining an added path, $p < .0001$. The $p$-values used to make these decisions were based on robust $\chi^2$ difference tests. 

In line with the exploratory analyses reported above, error covariances added using this method clustered among the six _negative emotionality_ items, providing further support for a two-factor model.^[These analyses are not reported but can be made available upon request.]

\pagebreak

```{r, eval=FALSE}
# Get loadings for final factor model
mod_matrix <- inspect(res_1[["fit_covar"]], what = "std")
F1_loadings <- mod_matrix$lambda[,"F1"][mod_matrix$lambda[,"F1"] != 0] %>% 
    round(digits = 3)
names(F1_loadings) <- c(emo_distress, high_stress)

# Get fit stats for final factor model
fit_tbl <- fitmeasures(res_1[["fit_covar"]], fit.measures = fit_stats) %>% 
  round(digits = 3)
names(fit_tbl) <- c('$\\chi^2$', '$df$', '$p$', 'CFI', 'SRMR')

# Get model comparison table
anv_tbl <- anova(res_1[["base_fit"]], res_1[["fit_covar"]])
anv_tbl <- anv_tbl[,c('Df', 'Chisq', 'Chisq diff', 'Pr(>Chisq)')] %>% 
  round(digits = 3)
rownames(anv_tbl) <- c("Residualized", "Base Model")

knitr::kable(fit_tbl, booktabs = TRUE, format = "latex", 
             caption = str_wrap("Fit statistics for single factor CFA model, with error covariances"))

knitr::kable(F1_loadings, booktabs = TRUE, format = "latex", col.names = c("F1"),
             caption = str_wrap("Standardized factor loadings for Factor 1, from residualized model"))

knitr::kable(anv_tbl, booktabs = TRUE, format = "latex", 
             caption = str_wrap("Comparison of single factor base model with single factor residualized model"))
```

\pagebreak

\newthought{Assessing the Reliablity} of a latent variable requires additional calculations. Specifically, we extracted estimates of Cronbach's $\alpha$, McDonald's $\omega$, Hancock's coefficient $H$, and average variance extracted from the single factor model (AVE). Cronbach's $\alpha$, McDonald's $\omega$, and Hancock's coefficient $H$ address the degree to which a latent factor is internally consistent, reliable, and replicable. Combined, these three metrics provide convergent evidence _wrt_ the degree to which we can expect the same set of items to comprise a latent factor in separate samples drawn from the same population. In general, higher values indicate more consistent, reliable, and replicable latent dimensions.

AVE stands slightly apart from the other three metrics. It represents the amount of variance in the observed variables accounted for by the latent factor. When a latent dimension accounts for a high level of average variance ($> .5$) in its observed indicator variables, there is stronger evidence for the creation of a composite score from the observed data, particularly when combined with a theoretical rationale for aggregating scores. 

Results suggest that despite the mediocre data-model fit, a single factor model yielded a consistent, reliable, and replicable dimension that accounted for a reasonably large amount of average variance in the observed ratings. 

```{r}
H_vals <- coefficient_H(mod_matrix$lambda)
rel_stats <- semTools::reliability(res_1[["base_fit"]])
rel_stats <- c(rel_stats[c('alpha', 'omega3', 'avevar'),], H_vals) %>% 
  as.matrix() %>% 
  round(digits = 3)

rownames(rel_stats) <- c("$\\alpha$", "$\\omega$", "AVE", "$H$")

knitr::kable(rel_stats, booktabs = TRUE, format = "latex", col.names = c('F1'),
             caption = str_wrap("Reliability statistics of one-factor CFA model, with no error covariances. AVE = average variance extracted"), 
             escape = FALSE)
```

\pagebreak

## Two-Factor Model

\newthought{The two-fator model} was defined according to the factor structure implied by our exploratory analyses. We created a _negative emotionality_ factor (`F1`) and an _exhaustion_ factor (`F2`) and compared the data-model fit of this slightly more complex model to the "baseline" one-factor model. 

Focusing first on the global fit statistics for this model, both the $CFI$ and $SRMR$ values were acceptable _wrt_ their common comparison thresholds ($> .95$ and $< .08$ respectively). The robust $\chi^2$ test of model fit was significant, but, as we can see in Table 10, also much improved relative to the one-factor model. Lastly, the two-factor model returned a set of standardized loadings for each indicator that echoed those returned by the principal components analyses both in terms of magnitude and direction. 

```{r}
two_fact_base <- "
F1 =~ Q30A_hopeless_r_2wks + Q30D_lonely_r_2wks + Q30E_sad_r_2wks + Q30F_depressed_r_2wks + Q30G_anxiety_r_2wks + Q30H_anger_r_2wks 
F2 =~ Q30B_overwhelmed_r_2wks + Q30C_exhausted_r_2wks
"

valid_covars <- paste0(names(label_map)[label_map %in% emo_distress], '_2wks')

# Had to run through the comparisons and then add in iteratively error covariances that included heywood cases
invalid_covars <- c(
  "Q30D_lonely_r_2wks~~Q30G_anxiety_r_2wks"
)

res_2 <- find_dual_factor_final_model(two_fact_base, data = grad_train, ordered_vars = dich_vars, valid_covars = valid_covars, 
                                      invalid_covars = invalid_covars, p_thresh = 1e-5)

# Get loadings for base model
mod_matrix <- inspect(res_2[["base_fit"]], what = "std")
F_loadings <- mod_matrix$lambda %>% 
    round(digits = 3)
F_loadings[F_loadings == 0] <- NA
rownames(F_loadings) <- c(emo_distress, high_stress)

# Get fit stats for base model
fit_tbl <- fitmeasures(res_2[["base_fit"]], fit.measures = fit_stats) %>% 
  round(digits = 3)
names(fit_tbl) <- c('$\\chi^2$', '$df$', '$p$', 'CFI', 'SRMR')

# Get model comparison table
anv_tbl <- anova(res_1[["base_fit"]], res_2[["base_fit"]])
anv_tbl <- anv_tbl[,c('Df', 'Chisq', 'Chisq diff', 'Pr(>Chisq)')] %>% 
  round(digits = 3)
rownames(anv_tbl) <- c("Two-Factor", "One-Factor")
colnames(anv_tbl) <- c('$df$', '$\\chi^2$', '$\\Delta\\chi^2$', '$p$')

# Write tables
knitr::kable(fit_tbl, booktabs = TRUE, format = "latex", col.names = "Value", escape = FALSE,
             caption = str_wrap("Fit statistics for two-factor CFA model, with no error covariances."))

knitr::kable(F_loadings, booktabs = TRUE, format = "latex", col.names = c("F1", "F2"),
             caption = str_wrap("Standardized factor loadings for two-factor CFA model, with no error covariances."))

knitr::kable(anv_tbl, booktabs = TRUE, format = "latex", escape = FALSE,
             caption = str_wrap("Comparison of one-factor CFA model with two-factor CFA model, each with no error covariances."))
```

\pagebreak

\newthought{Using modification indices}, we analyzed the effect of adding covariances among the indicator items. Similar to the one-factor model, this modeling step was done for exploratory and diagnostic purposes. We used the same threshold ($p < .0001$) to retain error covariances when assessing the consequences of adding these paths _post hoc_. We did include one constraint that was not relevant for the one-factor model: covariances could only be added among items loading on the same factor. In practice, this meant that error covariances could only be added among those items that loaded on to the _negative emotionality_ factor. Adding a covariance between the errors for the two _exhaustion_ items would have been nonsensical.^[Nonsensical due to the fact that a latent variable already existed to account for the covariance between the two _exhaustion_ items, and they were the only two items to load onto it.] 

Examination of the final set of added error covariances suggested the possibility that a three-factor solution _may_ exist in which *hopelessness*, *sadness*, *loneliness*, and *depressed mood* load onto a *depressed affect* factor and a new *negative emotion intensity* factor comprising *anxiety* and *anger* is added.^[The results from these follow-ups are not provided here, but can be made available upon request.] The name of this third factor is somewhat challenging. The term *negative emotion intensity* was chosen to encapsulate this new factor for two reasons. First, anxiety and anger are more "energetic" emotions than a state such as sadness. Additionally, both items included the phrase "felt overwhelming anxiety/anger," which was not true of any other item in the set.

```{r, eval=FALSE}
# Get loadings for final factor model
mod_matrix <- inspect(res_2[["fit_covar"]], what = "std")

F_loadings <- mod_matrix$lambda %>% 
    round(digits = 3)
rownames(F_loadings) <- c(emo_distress, high_stress)

# Get fit stats for final factor model
fit_tbl <- fitmeasures(res_2[["fit_covar"]], fit.measures = fit_stats) %>% 
  round(digits = 3)
names(fit_tbl) <- c('$\\chi^2$', '$df$', '$p$', 'CFI', 'SRMR')

# Get model comparison table
anv_tbl <- anova(res_2[["base_fit"]], res_2[["fit_covar"]])
anv_tbl <- anv_tbl[,c('Df', 'Chisq', 'Chisq diff', 'Pr(>Chisq)')] %>% 
  round(digits = 3)
rownames(anv_tbl) <- c("Residualized", "Base Model")
colnames(anv_tbl) <- c('$df$', '$\\chi^2$', '$\\Delta\\chi^2$', '$p$')

knitr::kable(fit_tbl, booktabs = TRUE, format = "latex", col.names = "Value", escape = FALSE,
             caption = str_wrap("Fit statistics for two factor CFA model, with error covariances"))

knitr::kable(F_loadings, booktabs = TRUE, format = "latex", col.names = c("F1", "F2"),
             caption = str_wrap("Standardized factor loadings for two-factor, residualized model"))

knitr::kable(anv_tbl, booktabs = TRUE, format = "latex", escape = FALSE,
             caption = str_wrap("Comparison of two-factor base model with two-factor residualized model"))
```

\pagebreak

\newthought{The psychometric indicators} for the two-factor solution yielded acceptable values for consistency ($\alpha$), reliability ($\omega$), and replicability ($H$). Additionally, the two-factor solution returned a pair of latent variables that each accounted for more average variance in their respective items than the one-factor solution did. In conjunction with the model fit improvement, these statistics further support the assertion that the two-factor provided a better account for the observed data than the one-factor model, while returning sufficient evidence of factor reliability. 

```{r}
H_vals <- coefficient_H(mod_matrix$lambda)
rel_stats <- semTools::reliability(res_2[["base_fit"]])
rel_stats <- rbind(rel_stats[c('alpha', 'omega3', 'avevar'),], H_vals) %>% 
  as.matrix() %>% 
  round(digits = 3)

rownames(rel_stats) <- c("$\\alpha$", "$\\omega$", "AVE", "$H$")

knitr::kable(rel_stats, booktabs = TRUE, format = "latex", col.names = c('F1', 'F2'),
             caption = str_wrap("Reliability statistics of two-factor CFA model, with no error covariances."), 
             escape = FALSE)
```

\pagebreak

## Three-Factor Model

\newthought{We extended our} CFA models following examination of exploratory analyses performed on the two-factor model. Based on the principal components analyses we did not expect to find much evidence for a third factor in these data. Furthermore, dividing 8 inter-related items among three factors is challenging. However, as a sensitivity analysis we felt it important test for the possibility, especially after examining the pattern of retained error covariances in the two-factor model. 

As mentioned above, our exploratory approach to adding error covariances among items that loaded onto the same factor suggested that _negative emotionality_ may effectively split into two distinguishable factors: _depressed affect_ (*hopelessness*, *sadness*, *loneliness*, and *depressed mood*) and _negative emotion intensity_ (*anxiety* and *anger*). 

Our first approach to accounting for this divergence was to create a three-factor model and subject it to the same modeling process as was used for the one- and two-factor solutions. The results suggested that the three-factor model fit the data slightly better than the two-factor model. Both $CFI$ and $SRMR$ demonstrated incremental improvement, and $\Delta\chi^2$ was consistent with the three-factor model outperforming the two-factor model in reproducing the observed covariances (see Table 14).

```{r}
three_fact_base <- "
F1 =~ Q30A_hopeless_r_2wks + Q30D_lonely_r_2wks + Q30E_sad_r_2wks + Q30F_depressed_r_2wks 
F2 =~ Q30B_overwhelmed_r_2wks + Q30C_exhausted_r_2wks
F3 =~ Q30G_anxiety_r_2wks + Q30H_anger_r_2wks 
"
res_3 <- find_dual_factor_final_model(three_fact_base, data = grad_train, ordered_vars = dich_vars, valid_covars = valid_covars[1:4], 
                                      invalid_covars = NULL, p_thresh = 1e-5)

# Get loadings for base model
mod_matrix <- inspect(res_3[["base_fit"]], what = "std")
F_loadings <- mod_matrix$lambda %>% 
    round(digits = 3)
F_loadings[F_loadings == 0] <- NA
rownames(F_loadings) <- c(emo_distress[1:4], high_stress, emo_distress[5:6])

# Get fit stats for base model
fit_stats <- c('chisq.scaled', 'df', 'pvalue.scaled', 'cfi.scaled', 'srmr')
fit_tbl <- fitmeasures(res_3[["base_fit"]], fit.measures = fit_stats) %>% 
  round(digits = 3)
names(fit_tbl) <- c('$\\chi^2$', '$df$', '$p$', 'CFI', 'SRMR')

# Get model comparison table
anv_tbl <- anova(res_1[["base_fit"]], res_2[["base_fit"]], res_3[["base_fit"]])
anv_tbl <- anv_tbl[,c('Df', 'Chisq', 'Chisq diff', 'Pr(>Chisq)')] %>% 
  round(digits = 3)
rownames(anv_tbl) <- c("Three-Factor", "Two-Factor", "One Factor")
colnames(anv_tbl) <- c('$df$', '$\\chi^2$', '$\\Delta\\chi^2$', '$p$')

# Write tables
knitr::kable(fit_tbl, booktabs = TRUE, format = "latex", col.names = "Value", escape = FALSE,
             caption = str_wrap("Fit statistics for three-factor CFA model, with no error covariances."))

knitr::kable(F_loadings, booktabs = TRUE, format = "latex", col.names = c("F1", "F2", "F3"),
             caption = str_wrap("Standardized factor loadings for three-factor CFA model, with no error covariances."))

knitr::kable(anv_tbl, booktabs = TRUE, format = "latex", escape = FALSE,
             caption = str_wrap("Comparison of one-, two-, and three-factor CFA models, with no error covariances."))
```

```{r, eval=FALSE}
# Get loadings for final factor model
mod_matrix <- inspect(res_3[["fit_covar"]], what = "std")

F_loadings <- mod_matrix$lambda %>% 
    round(digits = 3)
rownames(F_loadings) <- c(emo_distress, high_stress)

# Get fit stats for final factor model
fit_tbl <- fitmeasures(res_3[["fit_covar"]], fit.measures = fit_stats) %>% 
  round(digits = 3)
names(fit_tbl) <- c('$\\chi^2$', '$df$', '$p$', 'CFI', 'SRMR')

# Get model comparison table
anv_tbl <- anova(res_3[["base_fit"]], res_3[["fit_covar"]])
anv_tbl <- anv_tbl[,c('Df', 'Chisq', 'Chisq diff', 'Pr(>Chisq)')] %>% 
  round(digits = 3)
rownames(anv_tbl) <- c("Residualized", "Base Model")
colnames(anv_tbl) <- c('$df$', '$\\chi^2$', '$\\Delta\\chi^2$', '$p$')

knitr::kable(fit_tbl, booktabs = TRUE, format = "latex", col.names = "Value", escape = FALSE,
             caption = str_wrap("Fit statistics for three-factor CFA model, with error covariances"))

knitr::kable(F_loadings, booktabs = TRUE, format = "latex", col.names = c("F1", "F2", "F3"),
             caption = str_wrap("Standardized factor loadings for three-factor CFA model, with error covariances"))

knitr::kable(anv_tbl, booktabs = TRUE, format = "latex", escape = FALSE,
             caption = str_wrap("Comparison of three-factor base model with three-factor residualized model"))
```

\pagebreak 

\newthought{Psychometric properties} of the three-factor model were mostly in line with the two-factor model. One notable difference is that $\alpha$, and $\omega$ both dropped below .80 and $H$ approached this commonly used threshold for the newly created third factor. AVE did, however, improve for `F1` when *anxiety* and *anger* were split off onto their own separate factor (`F3`). Thus, the evidence for adding this third factor based on these psychometric indices was mixed at best. Also noteworthy is the fact that the correlation between the *depressed affect* (`F1`) and *negative emotion intensity* (`F3`) was .891, suggesting that separating these factors may not provide much additional information in our analyses. In the next section - testing model fit using the hold out sample - we hoped to provide a more definitive assessment of the replication of each of these three models.  

```{r}
H_vals <- coefficient_H(mod_matrix$lambda)
rel_stats <- semTools::reliability(res_3[["base_fit"]])
rel_stats <- rbind(rel_stats[c('alpha', 'omega3', 'avevar'),], H_vals) %>% 
  as.matrix() %>% 
  round(digits = 3)

rownames(rel_stats) <- c("$\\alpha$", "$\\omega$", "AVE", "$H$")

knitr::kable(rel_stats, booktabs = TRUE, format = "latex", col.names = c('F1', 'F2', "F3"),
             caption = str_wrap("Reliability statistics of three-factor CFA model, with no error covariances."), 
             escape = FALSE)
```

\pagebreak

# Phase IV - Out of Sample CFA

As a first step in holdout sample testing, we created a series of models in which we fixed the loading, variance, and covariance estimates returned from each of the CFA models fit on the training data. We then fit these models with fixed parameters to the holdout testing data. Comparing global fit statistics, we found that, for the most part, each of the three models replicated in the holdout sample. Notably, the three-factor solution continued to offer the best data-model fit as indexed by $CFI$ and $SRMR$. 

At this stage, we can rule out the one-factor model as it consistently performed worse than both the two- and three-factor models. However, questions still remained regarding the interpretability, parsimony, and reliability of the three-factor model relative to the two-factor model. 

Given the high correlation between the *depressed affect* and *negative emotion intensity* factors detected in the three-factor model we proposed one additional factor structure in a final effort to reconcile all lessons learned from the data to this point. Specifically, we sought to create a model that: 

1. Incorporated two factors representing a "general" _negative emotionality_ dimension and an _exhaustion_ dimension in accordance with the PCA and correlational analyses documented in the first two sections. 
1. Allowed for a distinction between the _depressed affect_ item set and the two _negative emotion intensity_ items consistent with the three-factor solution. 

```{r}
for(var in dich_vars) {
  grad_test[[var]] <- as.factor(grad_test[[var]])
  grad_test[[var]] <- ordered(grad_test[[var]])
  print(levels(grad_test[[var]]))
}

grad_train[["train_flag"]] <- "Train"
grad_test[["train_flag"]] <- "Test"

grad_comb <- rbind(grad_train[,c(dich_vars, "train_flag")], grad_test[,c(dich_vars, "train_flag")])
```

```{r}
# Recreate fixed coefficients for model 1
params_1 <- parameterestimates(res_1$base_fit) %>% 
  filter(op == "=~" | op == "~~") %>% 
  select(lhs, op, est, rhs)

one_fact_test_fixed <- "{params_1[['lhs']]} {params_1[['op']]} {params_1[['est']]} * {params_1[['rhs']]}" %>% glue::glue()

fit_one_test_fixed <- cfa(one_fact_test_fixed, data=grad_test, ordered=dich_vars)
# summary(fit_one_test_fixed, standardized = TRUE, fit.measures=TRUE)
fit_stats_one_fixed <- fitmeasures(fit_one_test_fixed, fit.measures = fit_stats)

# Recreate fixed coefficients for model 2
params_2 <- parameterestimates(res_2$base_fit) %>% 
  # Take the unstandardized loadings and latent variable covariances and variances
  filter(op == "=~" | op == "~~") %>% 
  select(lhs, op, est, rhs)

two_fact_test_fixed <- "{params_2[['lhs']]} {params_2[['op']]} {params_2[['est']]} * {params_2[['rhs']]}" %>% glue::glue()

fit_two_test_fixed <- cfa(two_fact_test_fixed, data=grad_test, ordered=dich_vars)
# summary(fit_two_test_fixed, standardized = TRUE, fit.measures=TRUE)
fit_stats_two_fixed <- fitmeasures(fit_two_test_fixed, fit.measures = fit_stats)

# Recreate fixed coefficients for model 3
params_3 <- parameterestimates(res_3$base_fit) %>% 
  # Take the unstandardized loadings and latent variable covariances and variances
  filter(op == "=~" | op == "~~") %>% 
  select(lhs, op, est, rhs)

three_fact_test_fixed <- "{params_3[['lhs']]} {params_3[['op']]} {params_3[['est']]} * {params_3[['rhs']]}" %>% glue::glue()

fit_three_test_fixed <- cfa(three_fact_test_fixed, data=grad_test, ordered=dich_vars)
# summary(fit_three_test_fixed, standardized = TRUE, fit.measures=TRUE)
fit_stats_three_fixed <- fitmeasures(fit_three_test_fixed, fit.measures = fit_stats)

fit_tbl <- rbind(fit_stats_one_fixed, fit_stats_two_fixed, fit_stats_three_fixed) %>% 
  round(digits = 3)
rownames(fit_tbl) <- c("One Factor", "Two Factor", "Three Factor")
colnames(fit_tbl) <- c('$\\chi^2$', '$df$', '$p$', 'CFI', 'SRMR')

knitr::kable(fit_tbl, booktabs = TRUE, format = "latex", escape = FALSE,
             caption = str_wrap("Fit statistics for three replication models with loadings, variances, and covariances fixed to be equivalent to values returned from corresponding models fit on the training data set."))
```

\pagebreak 

# Final Proposed Model: A Hierarchical Negative Emotionality Model

Taking all evidence into account, we put forth a hybrid, hierarchical model below that incorporated all information gleaned from this set of analyses. Note that this model was equivalent in terms of fit statistics and degrees of freedom to the three-factor model. This final model has the benefit of retaining the two-factor structure that yielded both good model fit and good latent variable reliability estimates.^[When we extended to the three-factor model we observed some measures of consistency and reliability that fell below traditional cutoffs] The hierarchical model can also account for the slight, but detectable, divergence of the *anxiety* and *anger* items from the rest of the items tapping _negative emotionality_ (now `F4` in the hierarchical model). In our view, this factor structure represents the best description we can offer of the latent dimensions that underlie these data. 

Though not a particular focus of our main paper, we wanted to be certain we were on stable ground prior to the creation composite scores from this otherwise non-standardized item set. In the main report, we focused on the two factors at the top of the hierarchical structure: _negative emotionality_ (`F4`) and _exhaustion_ (`F2`) for the sake of parsimony.

```{r, fig.cap="Hierarchical model path structure. Note that the covariance between F2 and F4 is included in the model but is not depicted."}
hier_fact_base <- "
F1 =~ Q30A_hopeless_r_2wks + Q30D_lonely_r_2wks + Q30E_sad_r_2wks + Q30F_depressed_r_2wks 
F2 =~ Q30B_overwhelmed_r_2wks + Q30C_exhausted_r_2wks
F3 =~ Q30G_anxiety_r_2wks + Q30H_anger_r_2wks 
F4 =~ F1 + F3
"
dich_label_map <- label_map
names(dich_label_map) <- dich_vars[c(1,7,8,2:6)]
fit_hier_base <- cfa(hier_fact_base, data = grad_comb, ordered = dich_vars, std.lv=TRUE)
lavaanPlot::lavaanPlot(model = fit_hier_base, labels = dich_label_map, graph_options = list(fontsize = 9))
```

```{r}
mod_matrix <- inspect(fit_hier_base, what = "std")

beta_mat <- mod_matrix$beta %>% 
  data.frame() %>% 
  filter(F4 != 0)

F_loadings <- mod_matrix$lambda %>% 
  rbind(beta_mat) %>% 
  round(digits = 3)
rownames(F_loadings) <- c(emo_distress, high_stress, "F1", "F3")
F_loadings[F_loadings == 0] <- NA

knitr::kable(F_loadings, booktabs = TRUE, format = "latex", col.names = c("F1", "F2", "F3", "F4"),
             caption = str_wrap("Standardized factor loadings for hiearchical CFA model, with error no covariances."))
```

\pagebreak


# Packages and References

```{r bib, include=FALSE}
# create a bib file for the R packages used in this document
knitr::write_bib(c('base', 'rmarkdown', 'lavaan', 'tidyverse', 'psych', 'nFactors', 'lavaanPlot', 'semTools'), file = 'ACHA_factor_analysis.bib')
```
